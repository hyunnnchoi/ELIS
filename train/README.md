# ELIS Output Token Predictor - Training Module

BGE ê¸°ë°˜ Output Token ì˜ˆì¸¡ ëª¨ë¸ íŠ¸ë ˆì´ë‹ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

## ğŸ“‹ Overview

ì´ ëª¨ë“ˆì€ ë…¼ë¬¸ "ELIS: Efficient LLM Iterative Scheduling System with Response Length Predictor"ì— ì„¤ëª…ëœ Output Token Predictorë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.

### Architecture

```
Input Text (User Prompt + Generated Text)
    â†“
BGE Model (BAAI/bge-base-en-v1.5) - Frozen
    â†“
Mean Pooling (CLS + All Tokens)
    â†“
8 Fully Connected Layers (Hidden Dim: 1024, ReLU)
    â†“
Output: Predicted Remaining Tokens (Scalar)
```

### Key Features

- **Frozen BGE Embeddings**: Pre-trained BGE model parameters are frozen
- **Mean Pooling**: Uses all tokens (including CLS) for representation
- **8 FC Layers**: Hidden dimension of 1024 with ReLU activation
- **MSE Loss**: Regression loss for token count prediction
- **Metrics**: MAE (Mean Absolute Error) and RMSE (Root Mean Squared Error)

## ğŸš€ Quick Start

### 1. Install Dependencies

```bash
cd train
pip install -r requirements.txt
```

### 2. Run Training

**Basic Training:**
```bash
python train.py --data-dir ../data
```

**Custom Configuration:**
```bash
python train.py \
  --data-dir ../data \
  --batch-size 16 \
  --learning-rate 1e-4 \
  --epochs 16 \
  --hidden-dim 1024 \
  --num-layers 8 \
  --checkpoint-dir ./checkpoints
```

### 3. Monitor Training

Training will automatically:
- Split data into train/val/test (6:2:2)
- Train for specified epochs
- Save checkpoints and best model
- Evaluate on test set
- Save training history and results

## ğŸ“ File Structure

```
train/
â”œâ”€â”€ train.py          # Main training script
â”œâ”€â”€ model.py          # BGE + FC layers model
â”œâ”€â”€ dataset.py        # Data loading and preprocessing
â”œâ”€â”€ trainer.py        # Training loop and evaluation
â”œâ”€â”€ requirements.txt  # Dependencies
â”œâ”€â”€ README.md         # This file
â””â”€â”€ checkpoints/      # Auto-created for model checkpoints
    â”œâ”€â”€ best_model.pt
    â”œâ”€â”€ latest_model.pt
    â”œâ”€â”€ checkpoint_epoch_*.pt
    â”œâ”€â”€ training_history.json
    â””â”€â”€ test_results.json
```

## âš™ï¸ Configuration

### Command Line Arguments

#### Data Arguments
- `--data-dir`: Path to data directory (default: `../data`)
- `--max-length`: Maximum sequence length for BGE (default: `512`)

#### Model Arguments
- `--bge-model`: BGE model name (default: `BAAI/bge-base-en-v1.5`)
- `--hidden-dim`: Hidden dimension for FC layers (default: `1024`)
- `--num-layers`: Number of FC layers (default: `8`)
- `--no-freeze-bge`: Fine-tune BGE parameters instead of freezing

#### Training Arguments
- `--batch-size`: Training batch size (default: `16`)
- `--learning-rate`: Learning rate (default: `1e-4`)
- `--epochs`: Number of training epochs (default: `16`)
- `--early-stopping`: Early stopping patience (default: `5`)
- `--num-workers`: Number of data loader workers (default: `4`)

#### Checkpoint Arguments
- `--checkpoint-dir`: Directory to save checkpoints (default: `./checkpoints`)
- `--resume`: Path to checkpoint to resume from
- `--save-every`: Save checkpoint every N epochs (default: `1`)

#### Other Arguments
- `--seed`: Random seed for reproducibility (default: `42`)
- `--device`: Device to train on (default: auto-detect cuda/cpu)
- `--log-interval`: Logging interval in batches (default: `100`)

## ğŸ“Š Training Details

### Dataset

- **Source**: `ELIS/data/{model}/vllm_results_training.jsonl`
- **Models**: llama2-7b-hf, llama2-13b-hf, gpt-oss-20b, opt-6.7b, opt-13b, vicuna-13b-v1.5
- **Split**: 60% train, 20% validation, 20% test
- **Total Samples**: ~100K+ (varies by model)

### Data Format

Each training sample:
```json
{
  "input_prompt": "User question...",
  "output_prompt": "Generated text so far...",
  "number_of_output_tokens": 100,
  "remaining_tokens": 50  // LABEL
}
```

**Input**: `input_prompt + output_prompt` (full context seen by model)  
**Label**: `remaining_tokens` (how many tokens left to generate)

### Hyperparameters (ë…¼ë¬¸ ê¸°ì¤€)

| Parameter | Value |
|-----------|-------|
| Base Model | BAAI/bge-base-en-v1.5 |
| BGE Parameters | Frozen |
| FC Hidden Dim | 1024 |
| Number of FC Layers | 8 |
| Activation | ReLU |
| Loss Function | MSE |
| Optimizer | Adam |
| Learning Rate | 1Ã—10â»â´ |
| Batch Size | 16 |
| Epochs | 16 |
| Dataset Split | 6:2:2 |

## ğŸ“ˆ Output Files

### Checkpoints

- `best_model.pt`: Best model based on validation loss
- `latest_model.pt`: Most recent model checkpoint
- `checkpoint_epoch_N.pt`: Checkpoint at epoch N

### Training History

`training_history.json`:
```json
{
  "train_loss": [...],
  "train_mae": [...],
  "train_rmse": [...],
  "val_loss": [...],
  "val_mae": [...],
  "val_rmse": [...],
  "epoch_times": [...]
}
```

### Test Results

`test_results.json`:
```json
{
  "test_loss": 0.1234,
  "test_mae": 5.67,
  "test_rmse": 8.90,
  "best_epoch": 12,
  "best_val_loss": 0.1150
}
```

## ğŸ”§ Advanced Usage

### Resume Training

```bash
python train.py --resume ./checkpoints/latest_model.pt
```

### Fine-tune BGE Model

```bash
python train.py --no-freeze-bge
```

### Custom Model Configuration

```bash
python train.py \
  --hidden-dim 2048 \
  --num-layers 12 \
  --learning-rate 5e-5
```

### Multi-GPU Training

```bash
CUDA_VISIBLE_DEVICES=0,1,2,3 python train.py --batch-size 64
```

## ğŸ“Š Expected Results

ë…¼ë¬¸ì— ë³´ê³ ëœ ì„±ëŠ¥:
- Training converges around **epoch 16**
- MAE: ~**5-10 tokens** (ëª¨ë¸ì— ë”°ë¼ ë‹¤ë¦„)
- RMSE: ~**10-20 tokens** (ëª¨ë¸ì— ë”°ë¼ ë‹¤ë¦„)

## ğŸ› Troubleshooting

### Out of Memory (OOM)
```bash
# Reduce batch size
python train.py --batch-size 8

# Reduce max sequence length
python train.py --max-length 256
```

### Slow Training
```bash
# Increase number of workers
python train.py --num-workers 8

# Enable pin_memory (automatic for CUDA)
```

### Model Not Converging
```bash
# Adjust learning rate
python train.py --learning-rate 5e-5

# Increase epochs
python train.py --epochs 32
```

## ğŸ“ Notes

- BGE ëª¨ë¸ì€ ê¸°ë³¸ì ìœ¼ë¡œ frozenë˜ì–´ ìˆìœ¼ë©°, ì˜¤ì§ 8ê°œì˜ FC layerë§Œ í•™ìŠµë©ë‹ˆë‹¤.
- Mean poolingì€ CLS tokenê³¼ ëª¨ë“  ë‹¤ë¥¸ í† í°ë“¤ì„ í¬í•¨í•©ë‹ˆë‹¤.
- ë°ì´í„°ì…‹ì€ ì—¬ëŸ¬ LLM ëª¨ë¸ë“¤ì˜ ì¶œë ¥ì„ í†µí•©í•˜ì—¬ ì‚¬ìš©í•©ë‹ˆë‹¤.
- Random seed (42)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼ë¥¼ ë³´ì¥í•©ë‹ˆë‹¤.

## ğŸ“š Citation

```bibtex
@misc{choi2025elisefficientllmiterative,
      title={ELIS: Efficient LLM Iterative Scheduling System with Response Length Predictor}, 
      author={Seungbeom Choi and Jeonghoe Goo and Eunjoo Jeon and Mingyu Yang and Minsung Jang},
      year={2025},
      eprint={2505.09142},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      url={https://arxiv.org/abs/2505.09142}, 
}
```

